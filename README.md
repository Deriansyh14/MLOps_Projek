# ğŸš€ BERTopic MLOps - Complete Production MLOps Pipeline

A comprehensive MLOps project implementing topic modeling using BERTopic with production-ready infrastructure including model tracking, monitoring, CLI, testing, and deployment capabilities.

## ğŸ“‹ Project Overview

This project demonstrates a complete MLOps pipeline for topic modeling with:
- âœ… **Streamlit Web UI** for interactive analysis
- âœ… **Typer CLI** for command-line operations
- âœ… **MLflow Tracking & Registry** for experiment tracking and model versioning
- âœ… **Comprehensive Monitoring** with drift detection and coherence tracking
- âœ… **Full Test Suite** with pytest
- âœ… **CI/CD Pipeline** with GitHub Actions
- âœ… **Docker Support** for containerization

## ğŸ¯ Quick Start

### 1. Setup Environment

```bash
# Clone/download project
cd MLOps

# Create virtual environment (optional)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install typer CLI (if not in requirements)
pip install typer
```

### 2. Run Streamlit App

```bash
streamlit run app.py
```

The app will open at `http://localhost:8501` with 3 tabs:
- ğŸ¯ **Training & Analysis** - Upload CSV and train model
- ğŸ“Š **Model Monitoring** - Inference and drift detection
- ğŸ“ˆ **MLflow Tracking** - View experiment tracking info

### 3. Use CLI

```bash
# Show help
python -m cli.bertopic_cli --help

# Show version
python -m cli.bertopic_cli version

# Train model
python -m cli.bertopic_cli train --input data/raw/data.csv --max-trials 10

# Run inference
python -m cli.bertopic_cli predict --input data/test.csv

# Monitor performance
python -m cli.bertopic_cli monitor --input data/test.csv

# Evaluate experiments
python -m cli.bertopic_cli evaluate --metrics data/logs/experiment.json
```

### 4. Run Tests

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest tests/ --cov=backend --cov=cli

# Run specific test file
pytest tests/test_preprocessing.py -v

# Run only unit tests
pytest tests/ -k "test_" -v
```

### 5. View MLflow Tracking

```bash
# Start MLflow UI
mlflow ui --host 127.0.0.1 --port 5000

# Open http://localhost:5000 in browser
```

## ğŸ“ Project Structure

```
MLOps/
â”œâ”€â”€ app.py                          # Main Streamlit application
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ pytest.ini                      # Pytest configuration
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ DEPLOYMENT_CHECKLIST.md         # Compliance checklist
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ eda/
â”‚   â”‚   â””â”€â”€ eda_report.py          # EDA report generation
â”‚   â”œâ”€â”€ modeling/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ bertopic_analysis.py   # Core BERTopic analysis & MLflow tracking
â”‚   â”‚   â””â”€â”€ text_cleaning.py       # Text preprocessing
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ model_monitoring.py    # Drift detection & monitoring
â”‚   â””â”€â”€ registry/
â”‚       â””â”€â”€ model_registry.py      # Model registry functions
â”‚
â”œâ”€â”€ cli/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ run_bertopic.py            # Legacy CLI (deprecated)
â”‚   â””â”€â”€ bertopic_cli.py            # NEW: Typer-based CLI
â”‚
â”œâ”€â”€ mlops/
â”‚   â”œâ”€â”€ experiment_tracking.py     # Experiment logging
â”‚   â””â”€â”€ monitoring.py              # Basic monitoring utilities
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_preprocessing.py      # Text cleaning tests
â”‚   â”œâ”€â”€ test_bertopic_analysis.py  # Analysis & monitoring tests
â”‚   â””â”€â”€ test_cli.py                # CLI command tests
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Input data
â”‚   â”œâ”€â”€ processed/                 # Processed data
â”‚   â””â”€â”€ logs/                      # Experiment logs & monitoring data
â”‚
â”œâ”€â”€ save_models/
â”‚   â”œâ”€â”€ *.joblib                   # Saved model components
â”‚   â”œâ”€â”€ all-MiniLM-L6-v2/         # Pre-trained embeddings
â”‚   â””â”€â”€ nltk_data/                 # NLTK resources
â”‚
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ script.js                  # (Future) Custom JS
â”‚   â””â”€â”€ style.css                  # (Future) Custom CSS
â”‚
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ ml-ci.yml              # GitHub Actions CI/CD
```

## ğŸ”§ Key Components

### 1. **Model Training & Analysis** (`backend/modeling/bertopic_analysis.py`)

```python
from backend.modeling.bertopic_analysis import bertopic_analysis, generate_topics_with_label

# Automated hyperparameter tuning with coherence evaluation
result = bertopic_analysis(docs, max_trials=None)

# Generate final model with AI labels via Groq
topic_model, topic_info, topics, probs = generate_topics_with_label(
    docs=docs,
    embeddings=embeddings,
    embedding_model=embedding_model,
    umap_model=umap_model,
    vectorizer_model=vectorizer_model,
    ctfidf_model=ctfidf_model,
    min_cluster_size=best_size,
    groq_api_key=None  # Uses fallback word-based labels if API unavailable
)
```

**Features:**
- âœ… Lazy-loaded models (efficient memory usage)
- âœ… Coherence-based hyperparameter tuning
- âœ… MLflow tracking of metrics & artifacts
- âœ… Automatic model registry with versioning
- âœ… Groq API for intelligent topic labeling (with fallback)
- âœ… Deduplication of keywords

### 2. **Monitoring & Drift Detection** (`backend/monitoring/model_monitoring.py`)

```python
from backend.monitoring.model_monitoring import InferenceMonitor

# Initialize with trained model
monitor = InferenceMonitor(topic_model, baseline_topics)

# Run inference with drift detection
result = monitor.run_inference(
    new_docs=new_documents,
    new_texts_tokenized=tokenized_docs,
    dictionary=gensim_dictionary,
    calculate_coherence=True
)

# Get monitoring dashboard data
dashboard = monitor.get_monitoring_dashboard_data()
```

**Metrics Tracked:**
- Topic Drift (Jaccard similarity: 0-1)
- Coherence Score (c_v metric)
- Inference Time
- Document/Topic Counts
- Drift Level (LOW/MEDIUM/HIGH)

### 3. **CLI with Typer** (`cli/bertopic_cli.py`)

```bash
# Train model with options
bertopic train \
  --input data/documents.csv \
  --output save_models \
  --trials 10 \
  --experiment "experiment-name"

# Run inference
bertopic predict --input new_data.csv --model-version latest

# Monitor performance
bertopic monitor --input data/test.csv --output report.json

# View performance
bertopic evaluate --metrics data/logs/experiment.json

# Show version
bertopic version
```

### 4. **Comprehensive Testing** (`tests/`)

Test suites for:
- âœ… Text preprocessing & cleaning
- âœ… BERTopic analysis functions
- âœ… Monitoring & drift detection
- âœ… CLI commands
- âœ… Data validation

Run tests:
```bash
pytest tests/ -v --cov=backend --cov=cli
```

### 5. **Streamlit Application** (`app.py`)

**Tab 1: Training & Analysis**
- Upload CSV with Title & Abstract columns
- Automatic hyperparameter tuning
- Coherence score visualization
- Topic labels display
- Document explorer by topic
- CSV export

**Tab 2: Model Monitoring**
- New data inference
- Topic drift detection visualization
- Coherence trend analysis
- Performance metrics
- Real-time monitoring data

**Tab 3: MLflow Tracking**
- Experiment info
- Instructions for MLflow UI access

## ğŸ“Š Data Format

Input CSV must contain:
- `Title` (string) - Document title
- `Abstract` (string) - Document abstract

Example:
```csv
Title,Abstract
Antioxidant Compounds in Cancer Prevention,This study investigates...
Machine Learning Applications in Healthcare,Recent advances in ML...
```

## ğŸ” API Keys

**Groq API (optional for topic labeling):**
1. Get key from https://console.groq.com
2. Provide via:
   - Streamlit sidebar input
   - Environment variable: `GROQ_API_KEY`
   - Fallback: Auto-use word-based labels if API unavailable

## ğŸ“ˆ MLflow Integration

All training runs are automatically logged to MLflow:

```bash
# Start MLflow UI
mlflow ui --host 127.0.0.1 --port 5000
```

View:
- Experiment metrics (coherence, min_cluster_size, etc.)
- Artifacts (coherence plots, topic info CSV)
- Model versions (BERTopic-Model-v1, v2, etc.)
- Run parameters and timestamps

## âœ… Production Readiness Checklist

- âœ… **Model Serving**: Online inference via Streamlit + monitoring
- âœ… **CLI Interface**: Full Typer-based command-line support
- âœ… **Testing**: Comprehensive pytest suite with 20+ tests
- âœ… **Monitoring**: Topic drift detection, coherence tracking, performance metrics
- âœ… **Tracking**: MLflow experiment tracking & model registry
- âœ… **Documentation**: Complete README and deployment guide
- âœ… **Error Handling**: Graceful fallbacks for API failures
- âœ… **Logging**: Console + JSON logging + MLflow tracking

## ğŸš€ Deployment Options

### Option 1: Local Development
```bash
streamlit run app.py
```

### Option 2: Docker Container
```bash
docker build -t bertopic-mlops .
docker run -p 8501:8501 bertopic-mlops
```

### Option 3: Cloud Deployment
- **AWS EC2** - Host on EC2 instance with Streamlit
- **Heroku** - Deploy Streamlit app
- **Google Cloud Run** - Containerized deployment
- **Azure App Service** - Host on Azure

### Option 4: MLflow Model Registry
```python
import mlflow.pyfunc

# Load model from registry
model_uri = "models:/BERTopic-Model/latest"
model = mlflow.pyfunc.load_model(model_uri)
```

## ğŸ“– Documentation

- `README.md` - This file
- `DEPLOYMENT_CHECKLIST.md` - Compliance verification
- `MLOPS_ASSESSMENT.md` - Feature completeness assessment
- `pytest.ini` - Test configuration

## ğŸ¤ Contributing

1. Create feature branch
2. Add tests in `tests/`
3. Run test suite: `pytest tests/`
4. Submit PR

## ğŸ“ License

This project is provided as-is for educational purposes.

## â“ Support

For issues or questions:
1. Check DEPLOYMENT_CHECKLIST.md for requirements
2. Review test cases in `tests/` for usage examples
3. Check MLflow UI for experiment tracking

---

**Status**: âœ… Production Ready (v1.0.0)
**Last Updated**: December 2024
